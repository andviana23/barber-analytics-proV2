# Prometheus Alert Rules - Barber Analytics Pro
# Regras de alerta para monitoramento de SLA, performance e disponibilidade

groups:
  # ==============================================================================
  # GRUPO 1: UPTIME & AVAILABILITY
  # ==============================================================================
  - name: uptime_alerts
    interval: 5m
    rules:
      - alert: ServiceDowntime
        expr: avg_over_time(up{job="barber-backend"}[24h]) < 0.995
        for: 5m
        labels:
          severity: critical
          component: backend
          team: devops
        annotations:
          summary: "üî¥ CRITICAL: Barber Analytics com uptime < 99.5% (24h)"
          description: |
            O uptime do servi√ßo est√° abaixo de 99.5% nas √∫ltimas 24 horas.

            Valor atual: {{ $value | humanizePercentage }}
            Threshold: 99.5%
            Job: {{ $labels.job }}
            Instance: {{ $labels.instance }}

            A√ß√£o imediata necess√°ria! Verificar logs e infraestrutura.
          runbook_url: "https://docs.barberanalytics.com.br/runbooks/uptime-degradation"

      - alert: ServiceCompletelyDown
        expr: up{job="barber-backend"} == 0
        for: 1m
        labels:
          severity: critical
          component: backend
          team: devops
        annotations:
          summary: "üö® EMERG√äNCIA: Servi√ßo completamente fora do ar!"
          description: |
            O backend do Barber Analytics est√° COMPLETAMENTE FORA DO AR!

            Job: {{ $labels.job }}
            Instance: {{ $labels.instance }}

            A√á√ÉO IMEDIATA: Verificar servidor, processo PM2, e infraestrutura.
          runbook_url: "https://docs.barberanalytics.com.br/runbooks/service-down"

  # ==============================================================================
  # GRUPO 2: LATENCY & PERFORMANCE
  # ==============================================================================
  - name: latency_alerts
    interval: 1m
    rules:
      - alert: HighLatencyP95
        expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job="barber-backend"}[5m])) by (le)) > 0.5
        for: 5m
        labels:
          severity: warning
          component: backend
          team: backend
        annotations:
          summary: "‚ö†Ô∏è WARNING: Lat√™ncia p95 acima de 500ms (5 min)"
          description: |
            A lat√™ncia p95 das requisi√ß√µes HTTP est√° acima do threshold por mais de 5 minutos.

            Valor atual: {{ $value | humanizeDuration }}
            Threshold: 500ms
            Job: {{ $labels.job }}

            Poss√≠veis causas:
            - Queries lentas no banco de dados
            - Alto volume de requisi√ß√µes simult√¢neas
            - Recursos insuficientes (CPU/mem√≥ria)
            - N+1 queries n√£o otimizadas

            Verificar dashboard Backend Performance no Grafana.
          runbook_url: "https://docs.barberanalytics.com.br/runbooks/high-latency"

      - alert: HighLatencyP99
        expr: histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket{job="barber-backend"}[5m])) by (le)) > 1.0
        for: 5m
        labels:
          severity: critical
          component: backend
          team: backend
        annotations:
          summary: "üî¥ CRITICAL: Lat√™ncia p99 acima de 1s (5 min)"
          description: |
            A lat√™ncia p99 est√° acima de 1 segundo por mais de 5 minutos!

            Valor atual: {{ $value | humanizeDuration }}
            Threshold: 1s

            Isso afeta negativamente a experi√™ncia de 1% dos usu√°rios.
            Requer investiga√ß√£o imediata!
          runbook_url: "https://docs.barberanalytics.com.br/runbooks/critical-latency"

  # ==============================================================================
  # GRUPO 3: ERROR RATE
  # ==============================================================================
  - name: error_rate_alerts
    interval: 1m
    rules:
      - alert: HighErrorRate
        expr: |
          sum(rate(http_requests_total{job="barber-backend",status=~"5.."}[5m]))
          /
          sum(rate(http_requests_total{job="barber-backend"}[5m]))
          > 0.01
        for: 5m
        labels:
          severity: critical
          component: backend
          team: backend
        annotations:
          summary: "üî¥ CRITICAL: Taxa de erro 5xx acima de 1% (5 min)"
          description: |
            A taxa de erros 5xx est√° acima de 1% por mais de 5 minutos!

            Taxa atual: {{ $value | humanizePercentage }}
            Threshold: 1%
            Job: {{ $labels.job }}

            Poss√≠veis causas:
            - Bugs recentes em deploy
            - Banco de dados inacess√≠vel
            - Servi√ßos externos fora do ar (Asaas, etc)
            - Memory leak causando crashes

            A√á√ÉO IMEDIATA: Verificar logs do backend e considerar rollback.
          runbook_url: "https://docs.barberanalytics.com.br/runbooks/high-error-rate"

      - alert: High4xxErrorRate
        expr: |
          sum(rate(http_requests_total{job="barber-backend",status=~"4.."}[5m]))
          /
          sum(rate(http_requests_total{job="barber-backend"}[5m]))
          > 0.05
        for: 10m
        labels:
          severity: warning
          component: backend
          team: backend
        annotations:
          summary: "‚ö†Ô∏è WARNING: Taxa de erro 4xx acima de 5% (10 min)"
          description: |
            A taxa de erros 4xx est√° acima de 5% por mais de 10 minutos.

            Taxa atual: {{ $value | humanizePercentage }}
            Threshold: 5%

            Poss√≠veis causas:
            - Altera√ß√µes recentes na API quebrando contratos
            - Problemas de autentica√ß√£o/autoriza√ß√£o
            - Valida√ß√£o de entrada muito restritiva
            - Clients antigos fazendo requisi√ß√µes inv√°lidas

            Analisar logs para identificar endpoints problem√°ticos.
          runbook_url: "https://docs.barberanalytics.com.br/runbooks/client-errors"

  # ==============================================================================
  # GRUPO 4: CRON JOBS
  # ==============================================================================
  - name: cron_alerts
    interval: 5m
    rules:
      - alert: CronNotExecuted
        expr: (time() - cron_last_success_timestamp) > 90000
        for: 1m
        labels:
          severity: critical
          component: scheduler
          team: backend
        annotations:
          summary: "üî¥ CRITICAL: Cron job n√£o executou em 25+ horas"
          description: |
            O job {{ $labels.job_name }} n√£o executou com sucesso nas √∫ltimas 25 horas!

            Job: {{ $labels.job_name }}
            √öltima execu√ß√£o: {{ $value | humanizeDuration }} atr√°s
            Threshold: 25 horas

            Poss√≠veis causas:
            - Scheduler travado/parado
            - Job falhando silenciosamente
            - Depend√™ncias externas indispon√≠veis
            - Timeout excessivo

            Verificar logs do scheduler e dashboard Cron Jobs no Grafana.
          runbook_url: "https://docs.barberanalytics.com.br/runbooks/cron-failure"

      - alert: CronHighFailureRate
        expr: |
          sum(rate(cron_executions_total{status="failed"}[1h])) by (job_name)
          /
          sum(rate(cron_executions_total[1h])) by (job_name)
          > 0.1
        for: 30m
        labels:
          severity: warning
          component: scheduler
          team: backend
        annotations:
          summary: "‚ö†Ô∏è WARNING: Alta taxa de falha em cron job (10%+ em 1h)"
          description: |
            O job {{ $labels.job_name }} est√° falhando em mais de 10% das execu√ß√µes na √∫ltima hora.

            Job: {{ $labels.job_name }}
            Taxa de falha: {{ $value | humanizePercentage }}

            Investigar causa raiz antes que se torne cr√≠tico.
          runbook_url: "https://docs.barberanalytics.com.br/runbooks/cron-degradation"

  # ==============================================================================
  # GRUPO 5: DATABASE
  # ==============================================================================
  - name: database_alerts
    interval: 1m
    rules:
      - alert: HighDatabaseConnections
        expr: db_connections_in_use > 20
        for: 5m
        labels:
          severity: warning
          component: database
          team: backend
        annotations:
          summary: "‚ö†Ô∏è WARNING: Conex√µes ativas do banco acima de 20 (5 min)"
          description: |
            O n√∫mero de conex√µes ativas do PostgreSQL est√° acima do threshold.

            Conex√µes ativas: {{ $value }}
            Threshold: 20

            Poss√≠veis causas:
            - Alto volume de tr√°fego
            - Connection leak (conex√µes n√£o sendo fechadas)
            - Queries lentas bloqueando conex√µes
            - Pool size insuficiente

            Verificar dashboard Database no Grafana e logs de queries.
          runbook_url: "https://docs.barberanalytics.com.br/runbooks/high-db-connections"

      - alert: DatabaseConnectionsExhausted
        expr: db_connections_waiting > 5
        for: 2m
        labels:
          severity: critical
          component: database
          team: backend
        annotations:
          summary: "üî¥ CRITICAL: Pool de conex√µes do banco esgotado!"
          description: |
            H√° {{ $value }} requisi√ß√µes aguardando conex√µes dispon√≠veis!

            Conex√µes esperando: {{ $value }}
            Conex√µes abertas: {{ with query "db_connections_open" }}{{ . | first | value }}{{ end }}
            Conex√µes em uso: {{ with query "db_connections_in_use" }}{{ . | first | value }}{{ end }}

            Isto causa timeouts e degrada√ß√£o severa do servi√ßo!

            A√á√ÉO IMEDIATA:
            1. Identificar queries lentas bloqueando conex√µes
            2. Considerar aumentar pool size
            3. Verificar connection leaks no c√≥digo
          runbook_url: "https://docs.barberanalytics.com.br/runbooks/db-pool-exhausted"

      - alert: SlowDatabaseQueries
        expr: histogram_quantile(0.99, sum(rate(db_queries_duration_seconds_bucket[5m])) by (le)) > 1.0
        for: 10m
        labels:
          severity: warning
          component: database
          team: backend
        annotations:
          summary: "‚ö†Ô∏è WARNING: Queries lentas no banco (p99 > 1s em 10 min)"
          description: |
            O percentil 99 da dura√ß√£o das queries est√° acima de 1 segundo.

            Dura√ß√£o p99: {{ $value | humanizeDuration }}
            Threshold: 1s

            Poss√≠veis causas:
            - Falta de √≠ndices estrat√©gicos
            - N+1 queries
            - Tabelas com muitos dados sem particionamento
            - Lock contention

            Executar EXPLAIN ANALYZE nas queries mais lentas.
            Verificar dashboard Database ‚Üí Slow Queries no Grafana.
          runbook_url: "https://docs.barberanalytics.com.br/runbooks/slow-queries"

  # ==============================================================================
  # GRUPO 6: RESOURCES
  # ==============================================================================
  - name: resource_alerts
    interval: 1m
    rules:
      - alert: HighMemoryUsage
        expr: go_memstats_heap_inuse_bytes{job="barber-backend"} > 500000000
        for: 10m
        labels:
          severity: warning
          component: backend
          team: backend
        annotations:
          summary: "‚ö†Ô∏è WARNING: Uso de mem√≥ria heap acima de 500MB (10 min)"
          description: |
            O uso de mem√≥ria heap do backend est√° alto.

            Heap em uso: {{ $value | humanize1024 }}B
            Threshold: 500MB

            Poss√≠veis causas:
            - Memory leak
            - Caching excessivo em mem√≥ria
            - Processamento de dados muito grandes

            Monitorar se continua crescendo (poss√≠vel leak).
          runbook_url: "https://docs.barberanalytics.com.br/runbooks/high-memory"

      - alert: HighGoroutineCount
        expr: go_goroutines{job="barber-backend"} > 1000
        for: 5m
        labels:
          severity: warning
          component: backend
          team: backend
        annotations:
          summary: "‚ö†Ô∏è WARNING: N√∫mero de goroutines acima de 1000 (5 min)"
          description: |
            O n√∫mero de goroutines ativas est√° alto.

            Goroutines: {{ $value }}
            Threshold: 1000

            Poss√≠veis causas:
            - Goroutine leak (goroutines n√£o finalizando)
            - Alto volume de requisi√ß√µes concorrentes
            - Opera√ß√µes ass√≠ncronas n√£o gerenciadas corretamente

            Analisar goroutine profiles com pprof.
          runbook_url: "https://docs.barberanalytics.com.br/runbooks/goroutine-leak"
